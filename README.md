# <p align="center">Module 22 Challenge: Big Data/PySpark SQL
## Data Analytics assignment to determine key metrics about fictitious home sales data
### Overview
1. Read in the data
2. Answer questions about the data
### Features
Use `Spark` to:
1. Create temporary views of the data
2. Partition the data
3. Cache (and uncache) a temporary table
### Prerequisites
- Familiarity with and use of the Python programming language, and software to interact with .ipynb files, such as [Jupyter Notebook](https://jupyter.org/)
- Familiarity with and use of [Spark SQL](https://spark.apache.org/sql/)
### Usage
- Download the contents of the repo (as they are) to the same directory
- Launch Jupyter Notebook, navigate to the appropriate directory, and open Home_Sales.ipynb
- If desired, clear outputs and inspect/run the code cell by cell, paying attention to prompts and comments throughout
- Have fun exploring/playing around with the data/code! What are some ways the code could be made more clear or efficient?
### License
[MIT License](https://opensource.org/licenses/MIT)
### Contact
[Email](mailto:cengelhart@gmail.com)\
[GitHub](https://github.com/cengelhart0120)